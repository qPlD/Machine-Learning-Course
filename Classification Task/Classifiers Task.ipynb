{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "###########################################################################\n",
    "# TRY SUBMITTING \n",
    "# PCA and Variation for SVC\n",
    "\n",
    "# FOR REGRESSION\n",
    "# Ridge Elastic Lasso Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data\n",
    "X_train = np.loadtxt('X_train.csv', delimiter=',', skiprows=1)\n",
    "X_test = np.loadtxt('X_test.csv', delimiter=',', skiprows=1)\n",
    "y_train = np.loadtxt('y_train.csv', delimiter=',', skiprows=1)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange answer in two columns. First column (with header \"Id\") is an\n",
    "# enumeration from 0 to n-1, where n is the number of test points. Second\n",
    "# column (with header \"EpiOrStroma\" is the predictions.\n",
    "def saveFile(y_pred,name):\n",
    "    test_header = \"Id,EpiOrStroma\"\n",
    "    n_points = X_test.shape[0]\n",
    "    y_pred_pp = np.ones((n_points, 2))\n",
    "    y_pred_pp[:, 0] = range(n_points)\n",
    "    y_pred_pp[:, 1] = y_pred\n",
    "    np.savetxt(name, y_pred_pp, fmt='%d', delimiter=\",\",\n",
    "               header=test_header, comments=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 79  52  85 106  17   1  73  16 100  15  71  89  75  55  62  20   6  45\n",
      "  88  54  50  33   7  65  41 109  72  57  74  83  98 104  34  23  14 105\n",
      "  44  61 108   8  35  22  37  70  58  40  92  21  27  69  66  87  13  80\n",
      "  78  95  68  43  90  63   3  64  49  82  39 101   5  53   4  25  77  32\n",
      "  12  86  84  76 103  26  81  51  94  59  67   2 110  48  38  19  42  29\n",
      "  56  31  46  30  99  11  91   9  47  10  28  97  96  93 111  36  24 107\n",
      "  60 102   0  18]\n",
      "[ 79  52  85 106  17   1  73  16 100  15  71  89  75  55  62  20   6  45\n",
      "  88  54  50  33   7  65  41 109  72  57  74  83  98 104  34  23  14 105\n",
      "  44  61 108   8  35  22  37  70  58  40  92  21  27  69  66  87  13  80\n",
      "  78  95  68  43  90  63   3  64  49  82  39 101   5  53   4  25  77  32\n",
      "  12  86  84  76 103  26  81  51  94  59  67   2 110]\n"
     ]
    }
   ],
   "source": [
    "#Feature Selection (all models)\n",
    "'''\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=50)\n",
    "fit = bestfeatures.fit(X_train,y_train)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X_train.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(50,'Score')) \n",
    "'''\n",
    "\n",
    "\n",
    "#Feature Importance\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "feat_scores = model.feature_importances_\n",
    "#print(feat_scores) \n",
    "\n",
    "feat_indexes = np.argsort(feat_scores)\n",
    "top_indexes = feat_indexes[:85]\n",
    "print(feat_indexes)\n",
    "print(top_indexes)\n",
    "\n",
    "\n",
    "new_X_train = np.empty([len(X_train),len(top_indexes)])\n",
    "new_X_test = np.empty([len(X_test),len(top_indexes)])\n",
    "#new_y_train = np.empty([len(y_train)])\n",
    "\n",
    "#print(X_train.shape, y_train.shape)\n",
    "#print(new_X_train.shape, new_y_train.shape)\n",
    "#print(X_train.shape[0],y_train.shape[0])\n",
    "#print(new_X_train.shape[0],new_y_train.shape[0])\n",
    "\n",
    "counter = 0\n",
    "for index in top_indexes:\n",
    "    new_X_train[:,counter] = X_train[:,index]\n",
    "    new_X_test[:,counter] = X_test[:,index]\n",
    "    #new_y_train[counter] = y_train[index]\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 82) (200, 112)\n",
      "(798, 82) (798, 112)\n"
     ]
    }
   ],
   "source": [
    "#Remove Features with Low Variance (all models)\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "X_train2 = sel.fit_transform(X_train)\n",
    "X_test2 = sel.fit_transform(X_test)\n",
    "\n",
    "print(X_train2.shape, X_train.shape)\n",
    "print(X_test2.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 90) (200, 112)\n",
      "(798, 90) (798, 112)\n"
     ]
    }
   ],
   "source": [
    "# PCA \n",
    "pca = PCA(n_components=90)\n",
    "X_train3 = pca.fit_transform(X_train)\n",
    "X_test3 = pca.fit_transform(X_test)\n",
    "\n",
    "print(X_train3.shape, X_train.shape)\n",
    "print(X_test3.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression: 0.8228384794054174 \n",
      "(Kaggle: 0.36820)\n",
      "Linear Regression (Feat. Selection): 0.8436246969455491 \n",
      "(Kaggle: 0.)\n",
      "Linear Regression (Feat. Selection 2): 0.8085832361913172 \n",
      "(Kaggle: 0.)\n",
      "Linear Regression (PCA): 0.8064496058438628 \n",
      "(Kaggle: 0.)\n",
      "\n",
      "Transforemed Linear Regression: 0.9797550359348112 \n",
      "(Kaggle: 0.62761)\n",
      "Transforemed Linear Regression (Feat. Selection): 0.9797550359348112 \n",
      "(Kaggle: 0.)\n",
      "Transforemed Linear Regression (Feat. Selection 2): 0.9190201437392449 \n",
      "(Kaggle: 0.)\n",
      "Transforemed Linear Regression (PCA): 0.9392651078044336 \n",
      "(Kaggle: 0.)\n",
      "\n",
      "Logistic Regression: 0.61 \n",
      "(Kaggle: 0.59832)\n",
      "Logistic Regression (Feat. Selection): 0.61 \n",
      "(Kaggle: 0.)\n",
      "Logistic Regression (Feat. Selection 2): 0.61 \n",
      "(Kaggle: 0.)\n",
      "Logistic Regression (PCA): 0.835 \n",
      "(Kaggle: 0.)\n",
      "\n",
      "Ridge Regression: 0.8031060028098278 \n",
      "(Kaggle: 0.42677)\n",
      "Ridge Regression (Feat. Selection): 0.7939592598975257 \n",
      "(Kaggle: 0.)\n",
      "Ridge Regression (Feat. Selection) 2: 0.796431416130921 \n",
      "(Kaggle: 0.)\n",
      "\n",
      "SVC: 1.0 \n",
      "(Kaggle: 0.47280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2239: UserWarning: n_quantiles (1000) is greater than the total number of samples (200). n_quantiles is set to n_samples.\n",
      "  % (self.n_quantiles, n_samples))\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2239: UserWarning: n_quantiles (1000) is greater than the total number of samples (200). n_quantiles is set to n_samples.\n",
      "  % (self.n_quantiles, n_samples))\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2239: UserWarning: n_quantiles (1000) is greater than the total number of samples (200). n_quantiles is set to n_samples.\n",
      "  % (self.n_quantiles, n_samples))\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:2239: UserWarning: n_quantiles (1000) is greater than the total number of samples (200). n_quantiles is set to n_samples.\n",
      "  % (self.n_quantiles, n_samples))\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.07675e-19): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.10599e-19): result may not be accurate.\n",
      "  overwrite_a=True).T\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=2.3428e-19): result may not be accurate.\n",
      "  overwrite_a=True).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC (Feat. Selection): 1.0 \n",
      "(Kaggle: 0.)\n",
      "SVC (Feat. Selection): 1.0 \n",
      "(Kaggle: 0.)\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression ##############################################################################\n",
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"\\nLinear Regression:\",reg.score(X_train, y_train),\"\\n(Kaggle: 0.36820)\")\n",
    "saveFile(y_pred,'basicLinReg.csv')\n",
    "\n",
    "reg2 = LinearRegression().fit(new_X_train, y_train)\n",
    "y_pred = reg2.predict(new_X_test)\n",
    "print(\"Linear Regression (Feat. Selection):\",reg2.score(new_X_train, y_train),\"\\n(Kaggle: 0.)\")\n",
    "# 60 Features: 0.6965845503795097 \n",
    "# 80 Features: 0.7746426562345887 \n",
    "# 90 Features: 0.7886728742183744 \n",
    "# 100 Features: 0.8228482666617984 \n",
    "# 110 Features: 0.8436246969455491 \n",
    "reg3 = LinearRegression().fit(X_train2, y_train)\n",
    "y_pred = reg3.predict(X_test2)\n",
    "print(\"Linear Regression (Feat. Selection 2):\",reg3.score(X_train2, y_train),\"\\n(Kaggle: 0.)\")\n",
    "# Variance 0.90: 0.8085832361913172 \n",
    "# Variance 0.80: 0.7926576460719514 \n",
    "reg4 = LinearRegression().fit(X_train3, y_train)\n",
    "y_pred = reg4.predict(X_test3)\n",
    "print(\"Linear Regression (PCA):\",reg4.score(X_train3, y_train),\"\\n(Kaggle: 0.)\")\n",
    "# 80 Features: 0.7917244508641572 \n",
    "# 90 Features: 0.8064496058438628 \n",
    "# 100 Features: 0.8412367324820228 \n",
    "# 110 Features: 0.8495380381821245 \n",
    "\n",
    "#saveFile(y_pred,'basicLinReg.csv')\n",
    "\n",
    "\n",
    "# Transformed Linear Regression ###################################################################\n",
    "regressor = LinearRegression()\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "reg = TransformedTargetRegressor(regressor=regressor, transformer=transformer)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"\\nTransforemed Linear Regression:\",reg.score(X_train, y_train),\"\\n(Kaggle: 0.62761)\")\n",
    "saveFile(y_pred,'basicLinRegTrans.csv')\n",
    "\n",
    "reg2 = TransformedTargetRegressor(regressor=regressor, transformer=transformer)\n",
    "reg2.fit(new_X_train, y_train)\n",
    "y_pred = reg2.predict(new_X_test)\n",
    "print(\"Transforemed Linear Regression (Feat. Selection):\",reg2.score(new_X_train, y_train),\"\\n(Kaggle: 0.)\")\n",
    "# 60 Features: 0.6558356108917907 \n",
    "# 80 Features: 0.8785302156088672 \n",
    "# 90 Features: 0.8482536955108613 \n",
    "# 100 Features: 0.9190201437392449 \n",
    "# 110 Features: 0.9797550359348112 \n",
    "\n",
    "reg3 = TransformedTargetRegressor(regressor=regressor, transformer=transformer)\n",
    "reg3.fit(X_train2, y_train)\n",
    "y_pred = reg3.predict(X_test2)\n",
    "print(\"Transforemed Linear Regression (Feat. Selection 2):\",reg3.score(X_train2, y_train),\"\\n(Kaggle: 0.)\")\n",
    "# Variance 0.90: 0.9190201437392449 \n",
    "# Variance 0.80: 0.9190201437392449 \n",
    "reg4 = TransformedTargetRegressor(regressor=regressor, transformer=transformer)\n",
    "reg4.fit(X_train3, y_train)\n",
    "y_pred = reg4.predict(X_test3)\n",
    "print(\"Transforemed Linear Regression (PCA):\",reg4.score(X_train3, y_train),\"\\n(Kaggle: 0.)\")\n",
    "# 80 Features: 0.9190201437392449 \n",
    "# 90 Features: 0.9392651078044336 \n",
    "# 100 Features: 0.9797550359348112 \n",
    "# 110 Features: 0.9595100718696224\n",
    "\n",
    "#saveFile(y_pred,'LinRegTransFeatSelect.csv')\n",
    "\n",
    "# Logistic Regression ############################################################################\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"\\nLogistic Regression:\",clf.score(X_train, y_train),\"\\n(Kaggle: 0.59832)\")\n",
    "\n",
    "clf2 = LogisticRegression()\n",
    "clf2 = clf2.fit(new_X_train, y_train)\n",
    "y_pred = clf2.predict(new_X_test)\n",
    "print(\"Logistic Regression (Feat. Selection):\",clf2.score(new_X_train, y_train),\"\\n(Kaggle: 0.)\")\n",
    "\n",
    "clf3 = LogisticRegression()\n",
    "clf3 = clf3.fit(X_train2, y_train)\n",
    "y_pred = clf3.predict(X_test2)\n",
    "print(\"Logistic Regression (Feat. Selection 2):\",clf3.score(X_train2, y_train),\"\\n(Kaggle: 0.)\")\n",
    "\n",
    "clf4 = LogisticRegression()\n",
    "clf4 = clf4.fit(X_train3, y_train)\n",
    "y_pred = clf4.predict(X_test3)\n",
    "print(\"Logistic Regression (PCA):\",clf4.score(X_train3, y_train),\"\\n(Kaggle: 0.)\")\n",
    "# 80 Features:  0.835\n",
    "# 90 Features: 0.835 \n",
    "# 100 Features: 0.825 \n",
    "# 110 Features: 0.835\n",
    "\n",
    "\n",
    "# Ridge Regression ################################################################################\n",
    "reg = linear_model.Ridge()\n",
    "reg = reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"\\nRidge Regression:\",reg.score(X_train, y_train),\"\\n(Kaggle: 0.42677)\")\n",
    "saveFile(y_pred,'RidgeReg.csv')\n",
    "\n",
    "reg2 = linear_model.Ridge()\n",
    "reg2 = reg2.fit(new_X_train, y_train)\n",
    "y_pred = reg2.predict(new_X_test)\n",
    "print(\"Ridge Regression (Feat. Selection):\",reg2.score(new_X_train, y_train),\"\\n(Kaggle: 0.)\")\n",
    "# 60 Features: 0.678622066513977 \n",
    "# 80 Features: 0.7392222257479466 \n",
    "# 90 Features: 0.7576009814592887 \n",
    "# 100 Features: 0.778140536752562 \n",
    "# 110 Features: 0.7939592598975257\n",
    "\n",
    "reg3 = linear_model.Ridge()\n",
    "reg3 = reg3.fit(X_train2, y_train)\n",
    "y_pred = reg3.predict(X_test2)\n",
    "print(\"Ridge Regression (Feat. Selection) 2:\",reg3.score(X_train2, y_train),\"\\n(Kaggle: 0.)\")\n",
    "# Variance 0.90: 0.796431416130921\n",
    "# Variance 0.80: 0.7908871733635068 \n",
    "\n",
    "#saveFile(y_pred,'basicLinReg.csv')\n",
    "\n",
    "# SVC Classifier Implementation ###################################################################\n",
    "svc = SVC(gamma='auto')\n",
    "svc.fit(X_train, y_train)\n",
    "print(\"\\nSVC:\",svc.score(X_train, y_train),\"\\n(Kaggle: 0.47280)\")\n",
    "y_pred = svc.predict(X_test)\n",
    "saveFile(y_pred,'basicSVC.csv')\n",
    "\n",
    "reg2 = SVC(gamma='auto')\n",
    "reg2.fit(new_X_train, y_train)\n",
    "y_pred = reg2.predict(new_X_test)\n",
    "print(\"SVC (Feat. Selection):\",reg2.score(new_X_train, y_train),\"\\n(Kaggle: 0.)\")\n",
    "# 60 Features: 1.0\n",
    "# 80 Features: 1.0\n",
    "# 90 Features: 1.0\n",
    "# 100 Features: 1.0\n",
    "# 110 Features: 1.0\n",
    "\n",
    "reg3 = SVC(gamma='auto')\n",
    "reg3.fit(X_train2, y_train)\n",
    "y_pred = reg3.predict(X_test2)\n",
    "print(\"SVC (Feat. Selection):\",reg3.score(X_train2, y_train),\"\\n(Kaggle: 0.)\")\n",
    "# Variance 0.90: 1.0\n",
    "# Variance 0.85: 1.0\n",
    "# Variance 0.90: 1.0\n",
    "\n",
    "\n",
    "#saveFile(y_pred,'SVCFeatSelect.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "\n",
    "#Default = 0.61\n",
    "\n",
    "clf = LogisticRegression(C=0.01)#(random_state=0, solver='lbfgs',multi_class='multinomial')\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(clf.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training samples:  140\n",
      "# testing samples:  60\n"
     ]
    }
   ],
   "source": [
    "## SPLITTING\n",
    "X_trainSPLIT, X_testSPLIT, y_trainSPLIT, y_testSPLIT = train_test_split(X_train,y_train,test_size=0.3,random_state=0)\n",
    "\n",
    "print(\"# training samples: \", len(X_trainSPLIT))\n",
    "print(\"# testing samples: \", len(X_testSPLIT))\n",
    "\n",
    "\n",
    "## SCALING\n",
    "sc = StandardScaler()\n",
    "X_train_split_std = sc.fit_transform(X_trainSPLIT)\n",
    "X_test_split_std = sc.transform(X_testSPLIT)\n",
    "\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "X_test_scaled = sc.fit_transform(X_test)\n",
    "\n",
    "X_train_scaled_feat = sc.fit_transform(new_X_train)\n",
    "X_test_scaled_feat = sc.fit_transform(new_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C value: 0.001       Accuracy: 0.85\n",
      "C value: 0.01       Accuracy: 0.9\n",
      "C value: 0.1       Accuracy: 0.8666666666666667\n",
      "C value: 1       Accuracy: 0.8833333333333333\n",
      "C value: 10       Accuracy: 0.8333333333333334\n",
      "C value: 100       Accuracy: 0.85\n",
      "Score: 0.55\n",
      "Score: 0.565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Testing Data with different parameter values\n",
    "C_param_range = [0.001,0.01,0.1,1,10,100]\n",
    "for i in C_param_range:\n",
    "    \n",
    "    # Apply logistic regression model to training data\n",
    "    lr = LogisticRegression(penalty = 'l2', C = i,random_state = 0)\n",
    "    lr.fit(X_train_split_std,y_trainSPLIT)\n",
    "\n",
    "    # Predict using model\n",
    "    y_pred = lr.predict(X_test_split_std)\n",
    "    \n",
    "    lr2 = LogisticRegression(penalty = 'l2', C = i,random_state = 0)\n",
    "    lr2.fit(X_trainSPLIT,y_trainSPLIT)\n",
    "    \n",
    "    # Saving accuracy score in table\n",
    "    print(\"C value:\",i,\"      Accuracy:\",accuracy_score(y_testSPLIT,y_pred))\n",
    "    \n",
    "print(\"Score:\",lr.score(X_trainSPLIT, y_trainSPLIT))\n",
    "print(\"Score:\",lr2.score(X_train, y_train))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'warn', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'warn', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression - Further Tuning with Grid Search\n",
    "lr = LogisticRegression()\n",
    "# Look at parameters used by our regression\n",
    "print('Parameters currently in use:\\n')\n",
    "print(lr.get_params())\n",
    "\n",
    "#Creating the Random Grid\n",
    "pipe = Pipeline([('classifier' , LogisticRegression())])\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier' : [LogisticRegression()],\n",
    "     'classifier__penalty' : ['l1', 'l2'],\n",
    "    'classifier__C' : np.logspace(-4, 4, 20),\n",
    "    'classifier__solver' : ['liblinear', 'saga'],\n",
    "    'classifier__max_iter' : [2,5,10,15,25,50,100,150,200]},\n",
    "]\n",
    "\n",
    "#print(\"\\n\",param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 720 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 3032 tasks      | elapsed:   11.5s\n",
      "[Parallel(n_jobs=-1)]: Done 7200 out of 7200 | elapsed:   45.2s finished\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classifier': LogisticRegression(C=0.03359818286283781, class_weight=None, dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=10, multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                    random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'classifier__C': 0.03359818286283781,\n",
       " 'classifier__max_iter': 10,\n",
       " 'classifier__penalty': 'l1',\n",
       " 'classifier__solver': 'saga'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Model LOGISTIC REG\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "clf = GridSearchCV(pipe, param_grid = param_grid, cv = 10, verbose=True, n_jobs=-1)\n",
    "# Fit the random search model\n",
    "best_clf = clf.fit(X_train_scaled_feat, y_train)\n",
    "\n",
    "\n",
    "best_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Score: 0.61\n",
      "Tuning Score: 0.855\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Basic Score:\",clf.score(X_train, y_train))\n",
    "y_predBasic = clf.predict(X_test)\n",
    "\n",
    "print(\"Tuning Score:\",best_clf.score(X_train_scaled_feat, y_train))\n",
    "\n",
    "# 'FeatTuningLR.csv'       - 100 Feats = 0.85  = 0.84937 Kaggle\n",
    "# 'ScaledTuningLR.csv'                 = 0.85  = 0.84937 Kaggle\n",
    "\n",
    "# 'FeatScaledTuningLR.csv' - 110 Feats = 0.845 = 0.89121 Kaggle\n",
    "# 'FeatScaledTuningLR.csv' - 100 Feats = 0.85  = 0.88702 Kaggle\n",
    "# 'FeatScaledTuningLR.csv' - 90 Feats  = 0.87  = 0.89539 Kaggle\n",
    "# 'FeatScaledTuningLR.csv' - 85 Feats  = 0.88  = 0.92050 Kaggle\n",
    "# 'FeatScaledTuningLR.csv' - 80 Feats  = 0.845 = 0.89958 Kaggle\n",
    "# 'FeatScaledTuningLR.csv' - 70 Feats  = 0.87  = 0.88702 Kaggle\n",
    "\n",
    "\n",
    "\n",
    "y_pred = best_clf.predict(X_test_scaled_feat)\n",
    "saveFile(y_pred,'FeatScaledTuningLRTEST.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic SVC 1.0\n",
      "Scaled SVC 0.935\n",
      "Scaled SVC 0.93\n",
      "Parameters currently in use:\n",
      "\n",
      "{'C': 1.0, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 'auto_deprecated', 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# SVC Classifier Tuning\n",
    "svc = SVC().fit(X_train,y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "print(\"Basic SVC\",svc.score(X_train, y_train))\n",
    "\n",
    "svc2 = SVC().fit(X_train_scaled,y_train)\n",
    "y_pred = svc2.predict(X_test_scaled)\n",
    "print(\"Scaled SVC\",svc2.score(X_train_scaled, y_train))\n",
    "#saveFile(y_pred,'scaledSVC.csv')\n",
    "\n",
    "svc3 = SVC().fit(X_train_scaled_feat,y_train)\n",
    "y_pred = svc3.predict(X_test_scaled_feat)\n",
    "print(\"Scaled SVC\",svc3.score(X_train_scaled_feat, y_train))\n",
    "#saveFile(y_pred,'scaledSVCTopFeats.csv')\n",
    "# 110 Features: 0.93\n",
    "# 100 Features: 0.94\n",
    "# 90 Features: 0.925\n",
    "# 80 Features: 0.925\n",
    "\n",
    "# Look at parameters used by our regression\n",
    "print('Parameters currently in use:\\n')\n",
    "print(svc.get_params())\n",
    "\n",
    "#Creating the Random Grid\n",
    "#pipe = Pipeline([('classifier' , SVC())])\n",
    "\n",
    "param_grid = [\n",
    "    {'kernel' : ['linear','rbf'],\n",
    "     'gamma' :[1,0.1,0.001,0.0001],\n",
    "     'C' : [0.0001, 0.001,0.01, 0.1, 1, 10, 100, 1000]},\n",
    "]\n",
    "\n",
    "#print(\"\\n\",param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'gamma': 1, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "#svc = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=True, n_jobs=-1)\n",
    "svc = GridSearchCV(SVC(),param_grid,refit = True)\n",
    "# Fit the random search model\n",
    "best_svc = svc.fit(X_train_scaled_feat, y_train)\n",
    "\n",
    "\n",
    "best_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Score: 1.0\n",
      "Tuning Score: 0.935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quentin\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "svc = SVC().fit(X_train, y_train)\n",
    "\n",
    "print(\"Basic Score:\",svc.score(X_train, y_train))\n",
    "y_predBasic = svc.predict(X_test)\n",
    "#saveFile(y_predBasic,'basicSVC.csv')\n",
    "\n",
    "print(\"Tuning Score:\",best_svc.score(X_train_scaled_feat, y_train))\n",
    "y_pred = best_svc.predict(X_test_scaled_feat)\n",
    "saveFile(y_pred,'ScaledTunedFeatSVC.csv')\n",
    "\n",
    "# Tuned + Scaled = 0.865\n",
    "\n",
    "# Tuned + Scaled + Feat Selected 115 =  Kaggle\n",
    "# Tuned + Scaled + Feat Selected 110 = 0.88702 Kaggle\n",
    "# Tuned + Scaled + Feat Selected 105 = 0.86192 Kaggle\n",
    "# Tuned + Scaled + Feat Selected 100 = 0.86192 Kaggle\n",
    "# Tuned + Scaled + Feat Selected 90 = 0.85355 Kaggle\n",
    "# Tuned + Scaled + Feat Selected 80 = 0.84518 Kaggle\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
